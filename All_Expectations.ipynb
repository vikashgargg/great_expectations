{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5c14a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "746f7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "322b7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6f64176",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('ge').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b36b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa023bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-D172A4L:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default_great_expectations_spark_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f2830bbdf0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a5f8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb4bb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.option('header','true').csv('opening_balance_raw_20221116.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b429f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+---+------+-------------------+\n",
      "|               ID|            name|age|gender|              email|\n",
      "+-----------------+----------------+---+------+-------------------+\n",
      "|                1|     Peter Poole| 30|  Male|      po@zalowpo.az|\n",
      "|                2|   Herbert Payne| 46|Female|      zos@toilet.bn|\n",
      "|                3|      Callie Roy| 57|  Male|kapkapor@reckeho.ye|\n",
      "|                4|  Isabella Wolfe| 61|Female| atuigome@asedas.io|\n",
      "|                5|     Sara Knight| 53|Female| dudnukob@ucboki.tg|\n",
      "|                6| Raymond Herrera| 23|  Male|       tolpij@ni.lu|\n",
      "|                7|  Jeffery Brooks| 65|  Male|   anirus@ajjuhu.mw|\n",
      "|                8|  Lillie Barnett| 55|  Male|      besi@vodje.hr|\n",
      "|                9|    Emma Alvarez| 21|  Male|          ze@ebo.jm|\n",
      "|               10|  Estella Rhodes| 27|  Male|         ez@jewe.si|\n",
      "|               11|     Eugene West| 18|Female| hohobum@janucke.vn|\n",
      "|               12|     Ollie Blake| 57|  Male|       big@ipjus.dm|\n",
      "|               13|Catherine Oliver| 47|Female|  jakag@lozzivde.eu|\n",
      "|               14|       Vera Boyd| 64|Female|      hifi@ariwu.zm|\n",
      "|               15|   Dylan Hubbard| 65|  Male|        ipren@ra.ml|\n",
      "|               16|      Ada Barber| 65|  Male|      roromo@tum.mh|\n",
      "|               17|      Brett Ford| 46|  Male|       wob@rames.fm|\n",
      "|               18|  Estelle Abbott| 23|Female|   derekic@tokap.ug|\n",
      "|               19|  Derek Gonzalez| 26|Female|    kuhuza@ludoj.be|\n",
      "|               20|Caroline Patrick| 37|Female|      digid@feah.ax|\n",
      "+-----------------+----------------+---+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64761b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    " \n",
    "\n",
    "from ruamel import yaml\n",
    "from great_expectations import *\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.data_context.types.base import (\n",
    "    DataContextConfig,\n",
    "    FilesystemStoreBackendDefaults,\n",
    ")\n",
    " \n",
    "root_directory = r\"C:\\Users\\hp\\Desktop\\ge1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8c25aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Data Context\n",
    "data_context_config = DataContextConfig(\n",
    "    store_backend_defaults=FilesystemStoreBackendDefaults(\n",
    "        root_directory=root_directory\n",
    "    ),\n",
    ")\n",
    "context = BaseDataContext(project_config=data_context_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5cd38f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to instantiate class from config...\n",
      "\tInstantiating as a Datasource, since class_name is Datasource\n",
      "\tSuccessfully instantiated Datasource\n",
      "\n",
      "\n",
      "ExecutionEngine class name: SparkDFExecutionEngine\n",
      "Data Connectors:\n",
      "\tinsert_your_data_connector_name_here:RuntimeDataConnector\n",
      "\n",
      "\tAvailable data_asset_names (0 of 0):\n",
      "\t\tNote : RuntimeDataConnector will not have data_asset_names until they are passed in through RuntimeBatchRequest\n",
      "\n",
      "\tUnmatched data_references (0 of 0): []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_spark_datasource_config = {\n",
    "    \"name\": \"insert_your_datasource_name_here\",\n",
    "    \"class_name\": \"Datasource\",\n",
    "    \"execution_engine\": {\"class_name\": \"SparkDFExecutionEngine\"},\n",
    "    \"data_connectors\": {\n",
    "        \"insert_your_data_connector_name_here\": {\n",
    "            \"module_name\": \"great_expectations.datasource.data_connector\",\n",
    "            \"class_name\": \"RuntimeDataConnector\",\n",
    "            \"batch_identifiers\": [\n",
    "                \"some_key_maybe_pipeline_stage\",\n",
    "                \"some_other_key_maybe_run_id\",\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "}\n",
    "context.test_yaml_config(yaml.dump(my_spark_datasource_config))\n",
    " \n",
    "context.add_datasource(**my_spark_datasource_config)\n",
    " \n",
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=\"insert_your_datasource_name_here\",\n",
    "    data_connector_name=\"insert_your_data_connector_name_here\",\n",
    "    data_asset_name=\"<YOUR_MEANGINGFUL_NAME>\",  # This can be anything that identifies this data_asset for you\n",
    "    batch_identifiers={\n",
    "        \"some_key_maybe_pipeline_stage\": \"prod\",\n",
    "        \"some_other_key_maybe_run_id\": f\"my_run_name_{datetime.date.today().strftime('%Y%m%d')}\",\n",
    "    },\n",
    "    runtime_parameters={\"batch_data\": df},  \n",
    ")\n",
    "expectation_suite_name = \"insert_your_expectation_suite_name_here\"\n",
    "context.create_expectation_suite(\n",
    "    expectation_suite_name=expectation_suite_name, overwrite_existing=True\n",
    ")\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=expectation_suite_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad8c6514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a3e90891524fb68943cd40bd7b7a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b010aac5d008460490f75f3609b15d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cf7b5c694946988bd4f1dd6098d974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed19969c7a8a46e1a49b5620b73f1b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2774, in evaluate_json_test_v3_api\n",
      "    check_json_test_result(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2941, in check_json_test_result\n",
      "    assert result[\"result\"][\"unexpected_list\"] == value, (\n",
      "AssertionError: expected ['aaaa@a123.a', 'aaaa@a123.d', 'aaaa@a123.e'] but got ['@a123.com', '@a123.eu', '@a123.io']\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2a42fdc1b241eeb536b3a66de8a35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501a99661617430fbffd3f748a457590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678a759fb39447b6a527987e5e1c2e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2774, in evaluate_json_test_v3_api\n",
      "    check_json_test_result(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2878, in check_json_test_result\n",
      "    assert result[\"success\"] == value, f\"{result['success']} != {value}\"\n",
      "AssertionError: False != True\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b3f91daa1b4a3f8420582f15858e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb9f9d21d00461d8d463ae89a487a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 421, in resolve_metrics\n",
      "    ] = self.resolve_metric_bundle(metric_fn_bundle=metric_fn_bundle)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\sparkdf_execution_engine.py\", line 725, in resolve_metric_bundle\n",
      "    res = df.agg(*aggregate[\"column_aggregates\"]).collect()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 817, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o567.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2761, in evaluate_json_test_v3_api\n",
      "    result = getattr(validator, expectation_type)(**runtime_kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 582, in inst_expectation\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 538, in inst_expectation\n",
      "    validation_result = expectation.validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 1138, in validate\n",
      "    evr: ExpectationValidationResult = validator.graph_validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1045, in graph_validate\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1025, in graph_validate\n",
      "    ) = self._resolve_suite_level_graph_and_process_metric_evaluation_errors(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1182, in _resolve_suite_level_graph_and_process_metric_evaluation_errors\n",
      "    resolved_metrics, aborted_metrics_info = graph.resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 192, in resolve\n",
      "    ] = self._resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 298, in _resolve\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 268, in _resolve\n",
      "    self._execution_engine.resolve_metrics(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 424, in resolve_metrics\n",
      "    raise ge_exceptions.MetricResolutionError(\n",
      "great_expectations.exceptions.exceptions.MetricResolutionError: An error occurred while calling o567.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788088d08afa42d2a28c01e741d95409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 421, in resolve_metrics\n",
      "    ] = self.resolve_metric_bundle(metric_fn_bundle=metric_fn_bundle)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\sparkdf_execution_engine.py\", line 725, in resolve_metric_bundle\n",
      "    res = df.agg(*aggregate[\"column_aggregates\"]).collect()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 817, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o578.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 6) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2761, in evaluate_json_test_v3_api\n",
      "    result = getattr(validator, expectation_type)(**runtime_kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 582, in inst_expectation\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 538, in inst_expectation\n",
      "    validation_result = expectation.validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 1138, in validate\n",
      "    evr: ExpectationValidationResult = validator.graph_validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1045, in graph_validate\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1025, in graph_validate\n",
      "    ) = self._resolve_suite_level_graph_and_process_metric_evaluation_errors(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1182, in _resolve_suite_level_graph_and_process_metric_evaluation_errors\n",
      "    resolved_metrics, aborted_metrics_info = graph.resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 192, in resolve\n",
      "    ] = self._resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 298, in _resolve\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 268, in _resolve\n",
      "    self._execution_engine.resolve_metrics(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 424, in resolve_metrics\n",
      "    raise ge_exceptions.MetricResolutionError(\n",
      "great_expectations.exceptions.exceptions.MetricResolutionError: An error occurred while calling o578.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 6) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c78e388f17483cad5f65429c483572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 421, in resolve_metrics\n",
      "    ] = self.resolve_metric_bundle(metric_fn_bundle=metric_fn_bundle)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\sparkdf_execution_engine.py\", line 725, in resolve_metric_bundle\n",
      "    res = df.agg(*aggregate[\"column_aggregates\"]).collect()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 817, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o589.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 9) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2761, in evaluate_json_test_v3_api\n",
      "    result = getattr(validator, expectation_type)(**runtime_kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 582, in inst_expectation\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 538, in inst_expectation\n",
      "    validation_result = expectation.validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 1138, in validate\n",
      "    evr: ExpectationValidationResult = validator.graph_validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1045, in graph_validate\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1025, in graph_validate\n",
      "    ) = self._resolve_suite_level_graph_and_process_metric_evaluation_errors(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1182, in _resolve_suite_level_graph_and_process_metric_evaluation_errors\n",
      "    resolved_metrics, aborted_metrics_info = graph.resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 192, in resolve\n",
      "    ] = self._resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 298, in _resolve\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 268, in _resolve\n",
      "    self._execution_engine.resolve_metrics(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 424, in resolve_metrics\n",
      "    raise ge_exceptions.MetricResolutionError(\n",
      "great_expectations.exceptions.exceptions.MetricResolutionError: An error occurred while calling o589.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 9) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526dd950cb434f3684952ff325b4a975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 421, in resolve_metrics\n",
      "    ] = self.resolve_metric_bundle(metric_fn_bundle=metric_fn_bundle)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\sparkdf_execution_engine.py\", line 725, in resolve_metric_bundle\n",
      "    res = df.agg(*aggregate[\"column_aggregates\"]).collect()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 817, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o600.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 12) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2761, in evaluate_json_test_v3_api\n",
      "    result = getattr(validator, expectation_type)(**runtime_kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 582, in inst_expectation\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 538, in inst_expectation\n",
      "    validation_result = expectation.validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 1138, in validate\n",
      "    evr: ExpectationValidationResult = validator.graph_validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1045, in graph_validate\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1025, in graph_validate\n",
      "    ) = self._resolve_suite_level_graph_and_process_metric_evaluation_errors(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1182, in _resolve_suite_level_graph_and_process_metric_evaluation_errors\n",
      "    resolved_metrics, aborted_metrics_info = graph.resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 192, in resolve\n",
      "    ] = self._resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 298, in _resolve\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 268, in _resolve\n",
      "    self._execution_engine.resolve_metrics(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 424, in resolve_metrics\n",
      "    raise ge_exceptions.MetricResolutionError(\n",
      "great_expectations.exceptions.exceptions.MetricResolutionError: An error occurred while calling o600.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 12) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e060b8e8f329417eb0281025820515ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 421, in resolve_metrics\n",
      "    ] = self.resolve_metric_bundle(metric_fn_bundle=metric_fn_bundle)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\sparkdf_execution_engine.py\", line 725, in resolve_metric_bundle\n",
      "    res = df.agg(*aggregate[\"column_aggregates\"]).collect()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 817, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o611.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 15) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2761, in evaluate_json_test_v3_api\n",
      "    result = getattr(validator, expectation_type)(**runtime_kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 582, in inst_expectation\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 538, in inst_expectation\n",
      "    validation_result = expectation.validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 1138, in validate\n",
      "    evr: ExpectationValidationResult = validator.graph_validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1045, in graph_validate\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1025, in graph_validate\n",
      "    ) = self._resolve_suite_level_graph_and_process_metric_evaluation_errors(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1182, in _resolve_suite_level_graph_and_process_metric_evaluation_errors\n",
      "    resolved_metrics, aborted_metrics_info = graph.resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 192, in resolve\n",
      "    ] = self._resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 298, in _resolve\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 268, in _resolve\n",
      "    self._execution_engine.resolve_metrics(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 424, in resolve_metrics\n",
      "    raise ge_exceptions.MetricResolutionError(\n",
      "great_expectations.exceptions.exceptions.MetricResolutionError: An error occurred while calling o611.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 15) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395a6d115d874cf79f3f0094e05f8b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 421, in resolve_metrics\n",
      "    ] = self.resolve_metric_bundle(metric_fn_bundle=metric_fn_bundle)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\sparkdf_execution_engine.py\", line 725, in resolve_metric_bundle\n",
      "    res = df.agg(*aggregate[\"column_aggregates\"]).collect()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 817, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o622.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 18) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2761, in evaluate_json_test_v3_api\n",
      "    result = getattr(validator, expectation_type)(**runtime_kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 582, in inst_expectation\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 538, in inst_expectation\n",
      "    validation_result = expectation.validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 1138, in validate\n",
      "    evr: ExpectationValidationResult = validator.graph_validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1045, in graph_validate\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1025, in graph_validate\n",
      "    ) = self._resolve_suite_level_graph_and_process_metric_evaluation_errors(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1182, in _resolve_suite_level_graph_and_process_metric_evaluation_errors\n",
      "    resolved_metrics, aborted_metrics_info = graph.resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 192, in resolve\n",
      "    ] = self._resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 298, in _resolve\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 268, in _resolve\n",
      "    self._execution_engine.resolve_metrics(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 424, in resolve_metrics\n",
      "    raise ge_exceptions.MetricResolutionError(\n",
      "great_expectations.exceptions.exceptions.MetricResolutionError: An error occurred while calling o622.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 18) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1879f6896c2344919baa2868186471c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 421, in resolve_metrics\n",
      "    ] = self.resolve_metric_bundle(metric_fn_bundle=metric_fn_bundle)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\sparkdf_execution_engine.py\", line 725, in resolve_metric_bundle\n",
      "    res = df.agg(*aggregate[\"column_aggregates\"]).collect()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 817, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o633.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 21) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2761, in evaluate_json_test_v3_api\n",
      "    result = getattr(validator, expectation_type)(**runtime_kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 582, in inst_expectation\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 538, in inst_expectation\n",
      "    validation_result = expectation.validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 1138, in validate\n",
      "    evr: ExpectationValidationResult = validator.graph_validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1045, in graph_validate\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1025, in graph_validate\n",
      "    ) = self._resolve_suite_level_graph_and_process_metric_evaluation_errors(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1182, in _resolve_suite_level_graph_and_process_metric_evaluation_errors\n",
      "    resolved_metrics, aborted_metrics_info = graph.resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 192, in resolve\n",
      "    ] = self._resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 298, in _resolve\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 268, in _resolve\n",
      "    self._execution_engine.resolve_metrics(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 424, in resolve_metrics\n",
      "    raise ge_exceptions.MetricResolutionError(\n",
      "great_expectations.exceptions.exceptions.MetricResolutionError: An error occurred while calling o633.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 21) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633283de30774f248a7bcb26d3c3abf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 421, in resolve_metrics\n",
      "    ] = self.resolve_metric_bundle(metric_fn_bundle=metric_fn_bundle)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\sparkdf_execution_engine.py\", line 725, in resolve_metric_bundle\n",
      "    res = df.agg(*aggregate[\"column_aggregates\"]).collect()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 817, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o644.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 24) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\self_check\\util.py\", line 2761, in evaluate_json_test_v3_api\n",
      "    result = getattr(validator, expectation_type)(**runtime_kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 582, in inst_expectation\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 538, in inst_expectation\n",
      "    validation_result = expectation.validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 1138, in validate\n",
      "    evr: ExpectationValidationResult = validator.graph_validate(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1045, in graph_validate\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1025, in graph_validate\n",
      "    ) = self._resolve_suite_level_graph_and_process_metric_evaluation_errors(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validator.py\", line 1182, in _resolve_suite_level_graph_and_process_metric_evaluation_errors\n",
      "    resolved_metrics, aborted_metrics_info = graph.resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 192, in resolve\n",
      "    ] = self._resolve(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 298, in _resolve\n",
      "    raise err\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\validator\\validation_graph.py\", line 268, in _resolve\n",
      "    self._execution_engine.resolve_metrics(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\execution_engine\\execution_engine.py\", line 424, in resolve_metrics\n",
      "    raise ge_exceptions.MetricResolutionError(\n",
      "great_expectations.exceptions.exceptions.MetricResolutionError: An error occurred while calling o644.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 24) (DESKTOP-D172A4L executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "\n",
      "3 5 10 False\n",
      "Completeness checklist for ExpectColumnValuesToContainValidEmail (EXPERIMENTAL):\n",
      " ✔ Has a valid library_metadata object\n",
      "   Has a docstring, including a one-line short description\n",
      "   Has at least one positive and negative example case, and all test cases pass\n",
      "   Has core logic and passes tests on at least one Execution Engine\n",
      "   Passes all linting checks\n",
      "      inspect.getfile(impl) raised a TypeError (impl is a built-in class)\n",
      "   Has basic input validation and type checking\n",
      "      No validate_configuration method defined on subclass\n",
      " ✔ Has both statement Renderers: prescriptive and diagnostic\n",
      "   Has core logic that passes tests for all applicable Execution Engines and SQL dialects\n",
      "      Only 6 / 8 tests for pandas are passing\n",
      "        - Failing: negative_test_for_emails_with_no_leading_string, valid_emails\n",
      "      Only 0 / 8 tests for spark are passing\n",
      "        - Failing: negative_test_for_no_domain_name, negative_test_for_no_at_symbol, negative_test_for_ending_with_one_character, negative_test_for_emails_with_no_leading_string, pass_test, pass_test, valid_emails, invalid_emails\n",
      "   Has a full suite of tests, as determined by a code owner\n",
      "   Has passed a manual review by a code owner for code standards and style guides\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "import json\n",
    "import re  # regular expressions\n",
    "\n",
    "# !!! This giant block of imports should be something simpler, such as:\n",
    "from great_expectations import *\n",
    "from great_expectations.execution_engine import (\n",
    "    PandasExecutionEngine,\n",
    "    SparkDFExecutionEngine,\n",
    "    SqlAlchemyExecutionEngine,\n",
    ")\n",
    "from great_expectations.expectations.expectation import (\n",
    "    ColumnMapExpectation,\n",
    "    Expectation,\n",
    "    ExpectationConfiguration,\n",
    ")\n",
    "from great_expectations.expectations.metrics import (\n",
    "    ColumnMapMetricProvider,\n",
    "    column_condition_partial,\n",
    ")\n",
    "from great_expectations.expectations.registry import (\n",
    "    _registered_expectations,\n",
    "    _registered_metrics,\n",
    "    _registered_renderers,\n",
    ")\n",
    "from great_expectations.expectations.util import render_evaluation_parameter_string\n",
    "from great_expectations.render.renderer.renderer import renderer\n",
    "from great_expectations.render.types import RenderedStringTemplateContent\n",
    "from great_expectations.render.util import num_to_str, substitute_none_for_missing\n",
    "from great_expectations.validator.validator import Validator\n",
    "\n",
    "EMAIL_REGEX = r\"[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,7}$\"\n",
    "\n",
    "\n",
    "class ColumnValuesContainValidEmail(ColumnMapMetricProvider):\n",
    "    # This is the id string that will be used to reference your metric.\n",
    "    condition_metric_name = \"column_values.valid_email\"\n",
    "    condition_value_keys = ()\n",
    "\n",
    "    # This method defines the business logic for evaluating your metric when using a PandasExecutionEngine\n",
    "    @column_condition_partial(engine=PandasExecutionEngine)\n",
    "    def _pandas(cls, column, **kwargs):\n",
    "        def matches_email_regex(x):\n",
    "            if re.match(EMAIL_REGEX, str(x)):\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        return column.apply(lambda x: matches_email_regex(x) if x else False)\n",
    "\n",
    "    # This method defines the business logic for evaluating your metric when using a SqlAlchemyExecutionEngine\n",
    "    #     @column_condition_partial(engine=SqlAlchemyExecutionEngine)\n",
    "    #     def _sqlalchemy(cls, column, _dialect, **kwargs):\n",
    "    #         return column.in_([3])\n",
    "\n",
    "    # This method defines the business logic for evaluating your metric when using a SparkDFExecutionEngine\n",
    "    @column_condition_partial(engine=SparkDFExecutionEngine)\n",
    "    def _spark(cls, column, **kwargs):\n",
    "        return column.rlike(EMAIL_REGEX)\n",
    "\n",
    "\n",
    "# This class defines the Expectation itself\n",
    "# The main business logic for calculation lives here.\n",
    "class ExpectColumnValuesToContainValidEmail(ColumnMapExpectation):\n",
    "    # These examples will be shown in the public gallery, and also executed as unit tests for your Expectation\n",
    "    examples = [\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"fail_case_1\": [\"a123@something\", \"a123@something.\", \"a123.\"],\n",
    "                \"fail_case_2\": [\"aaaa.a123.co\", \"aaaa.a123.\", \"aaaa.a123.com\"],\n",
    "                \"fail_case_3\": [\"aaaa@a123.e\", \"aaaa@a123.a\", \"aaaa@a123.d\"],\n",
    "                \"fail_case_4\": [\"@a123.com\", \"@a123.io\", \"@a123.eu\"],\n",
    "                \"pass_case_1\": [\n",
    "                    \"a123@something.com\",\n",
    "                    \"vinod.km@something.au\",\n",
    "                    \"this@better.work\",\n",
    "                ],\n",
    "                \"pass_case_2\": [\n",
    "                    \"example@website.dom\",\n",
    "                    \"ex.ample@example.ex\",\n",
    "                    \"great@expectations.email\",\n",
    "                ],\n",
    "                \"valid_emails\": [\n",
    "                    \"Janedoe@company.org\",\n",
    "                    \"someone123@stuff.net\",\n",
    "                    \"mycompany@mycompany.com\",\n",
    "                ],\n",
    "                \"bad_emails\": [\"Hello, world!\", \"Sophia\", \"this should fail\"],\n",
    "            },\n",
    "            \"tests\": [\n",
    "                {\n",
    "                    \"title\": \"negative_test_for_no_domain_name\",\n",
    "                    \"exact_match_out\": False,\n",
    "                    \"include_in_gallery\": True,\n",
    "                    \"in\": {\"column\": \"fail_case_1\"},\n",
    "                    \"out\": {\n",
    "                        \"success\": False,\n",
    "                        \"unexpected_index_list\": [0, 1, 2],\n",
    "                        \"unexpected_list\": [\n",
    "                            \"a123@something\",\n",
    "                            \"a123@something.\",\n",
    "                            \"a123.\",\n",
    "                        ],\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"negative_test_for_no_at_symbol\",\n",
    "                    \"exact_match_out\": False,\n",
    "                    \"include_in_gallery\": True,\n",
    "                    \"in\": {\"column\": \"fail_case_2\"},\n",
    "                    \"out\": {\n",
    "                        \"success\": False,\n",
    "                        \"unexpected_index_list\": [0, 1, 2],\n",
    "                        \"unexpected_list\": [\n",
    "                            \"aaaa.a123.co\",\n",
    "                            \"aaaa.a123.\",\n",
    "                            \"aaaa.a123.com\",\n",
    "                        ],\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"negative_test_for_ending_with_one_character\",\n",
    "                    \"exact_match_out\": False,\n",
    "                    \"include_in_gallery\": True,\n",
    "                    \"in\": {\"column\": \"fail_case_3\"},\n",
    "                    \"out\": {\n",
    "                        \"success\": False,\n",
    "                        \"unexpected_index_list\": [0, 1, 2],\n",
    "                        \"unexpected_list\": [\n",
    "                            \"aaaa@a123.e\",\n",
    "                            \"aaaa@a123.a\",\n",
    "                            \"aaaa@a123.d\",\n",
    "                        ],\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"negative_test_for_emails_with_no_leading_string\",\n",
    "                    \"exact_match_out\": False,\n",
    "                    \"include_in_gallery\": True,\n",
    "                    \"in\": {\"column\": \"fail_case_4\"},\n",
    "                    \"out\": {\n",
    "                        \"success\": False,\n",
    "                        \"unexpected_index_list\": [0, 1, 2],\n",
    "                        \"unexpected_list\": [\n",
    "                            \"aaaa@a123.e\",\n",
    "                            \"aaaa@a123.a\",\n",
    "                            \"aaaa@a123.d\",\n",
    "                        ],\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"pass_test\",\n",
    "                    \"exact_match_out\": False,\n",
    "                    \"include_in_gallery\": True,\n",
    "                    \"in\": {\"column\": \"pass_case_1\"},\n",
    "                    \"out\": {\n",
    "                        \"success\": True,\n",
    "                        \"unexpected_index_list\": [],\n",
    "                        \"unexpected_list\": [],\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"pass_test\",\n",
    "                    \"exact_match_out\": False,\n",
    "                    \"include_in_gallery\": True,\n",
    "                    \"in\": {\"column\": \"pass_case_2\"},\n",
    "                    \"out\": {\n",
    "                        \"success\": True,\n",
    "                        \"unexpected_index_list\": [],\n",
    "                        \"unexpected_list\": [],\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"valid_emails\",\n",
    "                    \"exact_match_out\": False,\n",
    "                    \"include_in_gallery\": True,\n",
    "                    \"in\": {\"column\": \"valid_emails\"},\n",
    "                    \"out\": {\n",
    "                        \"success\": True,\n",
    "                        \"unexpected_index_list\": [],\n",
    "                        \"unexpected_list\": [],\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"invalid_emails\",\n",
    "                    \"exact_match_out\": False,\n",
    "                    \"include_in_gallery\": True,\n",
    "                    \"in\": {\"column\": \"bad_emails\"},\n",
    "                    \"out\": {\n",
    "                        \"success\": False,\n",
    "                        \"unexpected_index_list\": [0, 1, 2],\n",
    "                        \"unexpected_list\": [\n",
    "                            \"Hello, world!\",\n",
    "                            \"Sophia\",\n",
    "                            \"this should fail\",\n",
    "                        ],\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # This dictionary contains metadata for display in the public gallery\n",
    "    library_metadata = {\n",
    "        \"maturity\": \"experimental\",\n",
    "        \"tags\": [\"experimental\", \"column map expectation\"],\n",
    "        \"contributors\": [  # Github\n",
    "            \"@aworld1\",\n",
    "            \"@enagola\",\n",
    "            \"@spencerhardwick\",\n",
    "            \"@vinodkri1\",\n",
    "            \"@degulati\",\n",
    "            \"@ljohnston931\",\n",
    "            \"@rexboyce\",\n",
    "            \"@lodeous\",\n",
    "            \"@sophiarawlings\",\n",
    "            \"@vtdangg\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # This is the id string of the Metric used by this Expectation.\n",
    "    # For most Expectations, it will be the same as the `condition_metric_name` defined in your Metric class above.\n",
    "    map_metric = \"column_values.valid_email\"\n",
    "\n",
    "    # This is a list of parameter names that can affect whether the Expectation evaluates to True or False\n",
    "    # Please see {some doc} for more information about domain and success keys, and other arguments to Expectations\n",
    "    success_keys = ()\n",
    "\n",
    "    # This dictionary contains default values for any parameters that should have default values\n",
    "    default_kwarg_values = {}\n",
    "\n",
    "    # This method defines a question Renderer\n",
    "    # For more info on Renderers, see {some doc}\n",
    "    # !!! This example renderer should render RenderedStringTemplateContent, not just a string\n",
    "\n",
    "\n",
    "#     @classmethod\n",
    "#     @renderer(renderer_type=\"renderer.question\")\n",
    "#     def _question_renderer(\n",
    "#         cls, configuration, result=None, language=None, runtime_configuration=None\n",
    "#     ):\n",
    "#         column = configuration.kwargs.get(\"column\")\n",
    "#         mostly = configuration.kwargs.get(\"mostly\")\n",
    "\n",
    "#         return f'Do at least {mostly * 100}% of values in column \"{column}\" equal 3?'\n",
    "\n",
    "# This method defines an answer Renderer\n",
    "# !!! This example renderer should render RenderedStringTemplateContent, not just a string\n",
    "#     @classmethod\n",
    "#     @renderer(renderer_type=\"renderer.answer\")\n",
    "#     def _answer_renderer(\n",
    "#         cls, configuration=None, result=None, language=None, runtime_configuration=None\n",
    "#     ):\n",
    "#         column = result.expectation_config.kwargs.get(\"column\")\n",
    "#         mostly = result.expectation_config.kwargs.get(\"mostly\")\n",
    "#         regex = result.expectation_config.kwargs.get(\"regex\")\n",
    "#         if result.success:\n",
    "#             return f'At least {mostly * 100}% of values in column \"{column}\" equal 3.'\n",
    "#         else:\n",
    "#             return f'Less than {mostly * 100}% of values in column \"{column}\" equal 3.'\n",
    "\n",
    "# This method defines a prescriptive Renderer\n",
    "#     @classmethod\n",
    "#     @renderer(renderer_type=\"renderer.prescriptive\")\n",
    "#     @render_evaluation_parameter_string\n",
    "#     def _prescriptive_renderer(\n",
    "#         cls,\n",
    "#         configuration=None,\n",
    "#         result=None,\n",
    "#         language=None,\n",
    "#         runtime_configuration=None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "# !!! This example renderer should be shorter\n",
    "#         runtime_configuration = runtime_configuration or {}\n",
    "#         include_column_name = runtime_configuration.get(\"include_column_name\", True)\n",
    "#         include_column_name = (\n",
    "#             include_column_name if include_column_name is not None else True\n",
    "#         )\n",
    "#         styling = runtime_configuration.get(\"styling\")\n",
    "#         params = substitute_none_for_missing(\n",
    "#             configuration.kwargs,\n",
    "#             [\"column\", \"regex\", \"mostly\", \"row_condition\", \"condition_parser\"],\n",
    "#         )\n",
    "\n",
    "#         template_str = \"values must be equal to 3\"\n",
    "#         if params[\"mostly\"] is not None:\n",
    "#             params[\"mostly_pct\"] = num_to_str(\n",
    "#                 params[\"mostly\"] * 100, precision=15, no_scientific=True\n",
    "#             )\n",
    "#             # params[\"mostly_pct\"] = \"{:.14f}\".format(params[\"mostly\"]*100).rstrip(\"0\").rstrip(\".\")\n",
    "#             template_str += \", at least $mostly_pct % of the time.\"\n",
    "#         else:\n",
    "#             template_str += \".\"\n",
    "\n",
    "#         if include_column_name:\n",
    "#             template_str = \"$column \" + template_str\n",
    "\n",
    "#         if params[\"row_condition\"] is not None:\n",
    "#             (\n",
    "#                 conditional_template_str,\n",
    "#                 conditional_params,\n",
    "#             ) = parse_row_condition_string_pandas_engine(params[\"row_condition\"])\n",
    "#             template_str = conditional_template_str + \", then \" + template_str\n",
    "#             params.update(conditional_params)\n",
    "\n",
    "#         return [\n",
    "#             RenderedStringTemplateContent(\n",
    "#                 **{\n",
    "#                     \"content_block_type\": \"string_template\",\n",
    "#                     \"string_template\": {\n",
    "#                         \"template\": template_str,\n",
    "#                         \"params\": params,\n",
    "#                         \"styling\": styling,\n",
    "#                     },\n",
    "#                 }\n",
    "#             )\n",
    "#         ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ExpectColumnValuesToContainValidEmail().print_diagnostic_checklist()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "222d7852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11432eedf744436b111c58eb9b6a5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"meta\": {},\n",
       "  \"success\": false,\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"result\": {\n",
       "    \"element_count\": 100,\n",
       "    \"unexpected_count\": 1,\n",
       "    \"unexpected_percent\": 1.0,\n",
       "    \"partial_unexpected_list\": [\n",
       "      \"bocojheb@ulpeti.co.uk\"\n",
       "    ],\n",
       "    \"missing_count\": 0,\n",
       "    \"missing_percent\": 0.0,\n",
       "    \"unexpected_percent_total\": 1.0,\n",
       "    \"unexpected_percent_nonmissing\": 1.0\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validator.expect_column_values_to_contain_valid_email(column=\"email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "333bf2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as gx\n",
    "import os\n",
    "from great_expectations.profile.basic_dataset_profiler import BasicDatasetProfiler\n",
    "from great_expectations.profile.user_configurable_profiler import UserConfigurableProfiler\n",
    "from great_expectations.dataset.sparkdf_dataset import SparkDFDataset\n",
    "from great_expectations.render.renderer import *\n",
    "from great_expectations.render.view import DefaultJinjaPageView\n",
    "from great_expectations.data_context.util import *\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13b6afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context_config = DataContextConfig(\n",
    "    validation_operators={\n",
    "        \"action_list_operator\": {\n",
    "            \"class_name\": \"ActionListValidationOperator\",\n",
    "            \"action_list\": [\n",
    "                {\n",
    "                    \"name\": \"store_validation_result\",\n",
    "                    \"action\": {\"class_name\": \"StoreValidationResultAction\"},\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"store_evaluation_params\",\n",
    "                    \"action\": {\"class_name\": \"StoreEvaluationParametersAction\"},\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"update_data_docs\",\n",
    "                    \"action\": {\"class_name\": \"UpdateDataDocsAction\"},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "    store_backend_defaults=FilesystemStoreBackendDefaults(\n",
    "        root_directory=root_directory\n",
    "    ),\n",
    ")\n",
    "context = BaseDataContext(project_config=data_context_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a47dba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to instantiate class from config...\n",
      "\tInstantiating as a Datasource, since class_name is Datasource\n",
      "\tSuccessfully instantiated Datasource\n",
      "\n",
      "\n",
      "ExecutionEngine class name: SparkDFExecutionEngine\n",
      "Data Connectors:\n",
      "\tinsert_your_data_connector_name_here:RuntimeDataConnector\n",
      "\n",
      "\tAvailable data_asset_names (0 of 0):\n",
      "\t\tNote : RuntimeDataConnector will not have data_asset_names until they are passed in through RuntimeBatchRequest\n",
      "\n",
      "\tUnmatched data_references (0 of 0): []\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"meta\": {\n",
       "    \"great_expectations_version\": \"0.15.36\"\n",
       "  },\n",
       "  \"expectation_suite_name\": \"insert_your_expectation_suite_name_here\",\n",
       "  \"data_asset_type\": null,\n",
       "  \"expectations\": [],\n",
       "  \"ge_cloud_id\": null\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_spark_datasource_config = {\n",
    "    \"name\": \"insert_your_datasource_name_here\",\n",
    "    \"class_name\": \"Datasource\",\n",
    "    \"execution_engine\": {\"class_name\": \"SparkDFExecutionEngine\"},\n",
    "    \"data_connectors\": {\n",
    "        \"insert_your_data_connector_name_here\": {\n",
    "            \"module_name\": \"great_expectations.datasource.data_connector\",\n",
    "            \"class_name\": \"RuntimeDataConnector\",\n",
    "            \"batch_identifiers\": [\n",
    "                \"some_key_maybe_pipeline_stage\",\n",
    "                \"some_other_key_maybe_run_id\",\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "}\n",
    " \n",
    "context.test_yaml_config(yaml.dump(my_spark_datasource_config))\n",
    " \n",
    "context.add_datasource(**my_spark_datasource_config)\n",
    " \n",
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=\"insert_your_datasource_name_here\",\n",
    "    data_connector_name=\"insert_your_data_connector_name_here\",\n",
    "    data_asset_name=\"<YOUR_MEANGINGFUL_NAME>\",  # This can be anything that identifies this data_asset for you\n",
    "    batch_identifiers={\n",
    "        \"some_key_maybe_pipeline_stage\": \"prod\",\n",
    "        \"some_other_key_maybe_run_id\": f\"my_run_name_{datetime.date.today().strftime('%Y%m%d')}\",\n",
    "    },\n",
    "    runtime_parameters={\"batch_data\": df},  # Your dataframe goes here\n",
    ")\n",
    " \n",
    "expectation_suite_name = \"insert_your_expectation_suite_name_here\"\n",
    "context.create_expectation_suite(\n",
    "    expectation_suite_name=expectation_suite_name, overwrite_existing=True\n",
    ")\n",
    "# validator = context.get_validator(\n",
    "#     batch_request=batch_request,\n",
    "#     expectation_suite_name=expectation_suite_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d040b18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1c91c2e21d4a5aae28157294f1b86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An unexpected Exception occurred during data docs rendering.  Because of this error, certain parts of data docs will not be rendered properly and/or may not appear altogether.  Please use the trace, included in this message, to diagnose and repair the underlying issue.  Detailed information follows:\n",
      "        TypeError: \"object of type 'NoneType' has no len()\".  Traceback: \"Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\content_block.py\", line 107, in _render_list\n",
      "    result = content_block_fn(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\renderer.py\", line 14, in inner_func\n",
      "    return renderer_fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 118, in inner_func\n",
      "    ] = render_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\core\\expect_compound_columns_to_be_unique.py\", line 183, in _prescriptive_renderer\n",
      "    for idx in range(len(params[\"column_list\"]) - 1):\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\".\n",
      "An unexpected Exception occurred during data docs rendering.  Because of this error, certain parts of data docs will not be rendered properly and/or may not appear altogether.  Please use the trace, included in this message, to diagnose and repair the underlying issue.  Detailed information follows:\n",
      "        TypeError: \"object of type 'NoneType' has no len()\".  Traceback: \"Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\content_block.py\", line 100, in _render_list\n",
      "    result = content_block_fn(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\validation_results_table_content_block.py\", line 117, in row_generator_fn\n",
      "    expectation_string_cell = expectation_string_fn(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\renderer.py\", line 14, in inner_func\n",
      "    return renderer_fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 118, in inner_func\n",
      "    ] = render_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\core\\expect_compound_columns_to_be_unique.py\", line 183, in _prescriptive_renderer\n",
      "    for idx in range(len(params[\"column_list\"]) - 1):\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\".\n",
      "An unexpected Exception occurred during data docs rendering.  Because of this error, certain parts of data docs will not be rendered properly and/or may not appear altogether.  Please use the trace, included in this message, to diagnose and repair the underlying issue.  Detailed information follows:\n",
      "                AttributeError: \"'NoneType' object has no attribute 'expectation_type'\".  Traceback: \"Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\content_block.py\", line 100, in _render_list\n",
      "    result = content_block_fn(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\validation_results_table_content_block.py\", line 117, in row_generator_fn\n",
      "    expectation_string_cell = expectation_string_fn(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\renderer.py\", line 14, in inner_func\n",
      "    return renderer_fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\expectation.py\", line 118, in inner_func\n",
      "    ] = render_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\expectations\\core\\expect_compound_columns_to_be_unique.py\", line 183, in _prescriptive_renderer\n",
      "    for idx in range(len(params[\"column_list\"]) - 1):\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\site_builder.py\", line 462, in build\n",
      "    rendered_content = self.renderer_class.render(resource)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\page_renderer.py\", line 125, in render\n",
      "    sections = self._collect_rendered_document_content_sections(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\page_renderer.py\", line 266, in _collect_rendered_document_content_sections\n",
      "    sections += [\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\page_renderer.py\", line 267, in <listcomp>\n",
      "    self._column_section_renderer.render(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\column_section_renderer.py\", line 486, in render\n",
      "    remaining_evrs, content_block = self._render_table(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\column_section_renderer.py\", line 474, in _render_table\n",
      "    new_block = self._table_renderer.render(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\content_block.py\", line 64, in render\n",
      "    result = render_fn(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\content_block.py\", line 125, in _render_list\n",
      "    result = content_block_fn(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\validation_results_table_content_block.py\", line 117, in row_generator_fn\n",
      "    expectation_string_cell = expectation_string_fn(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\validation_results_table_content_block.py\", line 239, in expectation_string_fn_with_legacy_translation\n",
      "    return legacy_expectation_string_fn(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\great_expectations\\render\\renderer\\content_block\\expectation_string.py\", line 30, in _missing_content_block_fn\n",
      "    \"expectation_type\": configuration.expectation_type,\n",
      "AttributeError: 'NoneType' object has no attribute 'expectation_type'\n",
      "\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"success\": false,\n",
      "  \"run_id\": {\n",
      "    \"run_time\": \"2022-12-14T12:07:16.670834+00:00\",\n",
      "    \"run_name\": \"20221214T120716.670834Z\"\n",
      "  },\n",
      "  \"run_results\": {\n",
      "    \"ValidationResultIdentifier::custom_expec_test/20221214T120716.670834Z/20221214T120716.670834Z/68a74190e9f8d93771db1ff953848ddd\": {\n",
      "      \"validation_result\": {\n",
      "        \"meta\": {\n",
      "          \"great_expectations_version\": \"0.15.36\",\n",
      "          \"expectation_suite_name\": \"custom_expec_test\",\n",
      "          \"run_id\": {\n",
      "            \"run_time\": \"2022-12-14T12:07:16.670834+00:00\",\n",
      "            \"run_name\": \"20221214T120716.670834Z\"\n",
      "          },\n",
      "          \"batch_spec\": {\n",
      "            \"data_asset_name\": \"<YOUR_MEANGINGFUL_NAME>\",\n",
      "            \"batch_data\": \"SparkDataFrame\"\n",
      "          },\n",
      "          \"batch_markers\": {\n",
      "            \"ge_load_time\": \"20221214T120716.490848Z\"\n",
      "          },\n",
      "          \"active_batch_definition\": {\n",
      "            \"datasource_name\": \"insert_your_datasource_name_here\",\n",
      "            \"data_connector_name\": \"insert_your_data_connector_name_here\",\n",
      "            \"data_asset_name\": \"<YOUR_MEANGINGFUL_NAME>\",\n",
      "            \"batch_identifiers\": {\n",
      "              \"some_key_maybe_pipeline_stage\": \"prod\",\n",
      "              \"some_other_key_maybe_run_id\": \"my_run_name_20221214\"\n",
      "            }\n",
      "          },\n",
      "          \"validation_time\": \"20221214T120716.671834Z\",\n",
      "          \"checkpoint_name\": null,\n",
      "          \"validation_id\": null,\n",
      "          \"checkpoint_id\": null\n",
      "        },\n",
      "        \"success\": false,\n",
      "        \"evaluation_parameters\": {},\n",
      "        \"results\": [\n",
      "          {\n",
      "            \"meta\": {},\n",
      "            \"success\": false,\n",
      "            \"exception_info\": {\n",
      "              \"exception_traceback\": \"Traceback (most recent call last):\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\execution_engine\\\\execution_engine.py\\\", line 407, in resolve_metrics\\n    resolved_metrics[metric_to_resolve.id] = metric_fn(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\metrics\\\\metric_provider.py\\\", line 55, in inner_func\\n    return metric_fn(*args, **kwargs)\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\metrics\\\\map_metric_provider.py\\\", line 481, in inner_func\\n    column_name = get_dbms_compatible_column_names(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\metrics\\\\util.py\\\", line 640, in get_dbms_compatible_column_names\\n    verify_column_names_exist(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\metrics\\\\util.py\\\", line 698, in verify_column_names_exist\\n    raise ge_exceptions.InvalidMetricAccessorDomainKwargsKeyError(\\ngreat_expectations.exceptions.exceptions.InvalidMetricAccessorDomainKwargsKeyError: Error: The column \\\"ID\\\" in BatchData does not exist.\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\validator\\\\validation_graph.py\\\", line 268, in _resolve\\n    self._execution_engine.resolve_metrics(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\execution_engine\\\\execution_engine.py\\\", line 411, in resolve_metrics\\n    raise ge_exceptions.MetricResolutionError(\\ngreat_expectations.exceptions.exceptions.MetricResolutionError: Error: The column \\\"ID\\\" in BatchData does not exist.\\n\",\n",
      "              \"exception_message\": \"Error: The column \\\"ID\\\" in BatchData does not exist.\",\n",
      "              \"raised_exception\": true\n",
      "            },\n",
      "            \"result\": {},\n",
      "            \"expectation_config\": {\n",
      "              \"meta\": {},\n",
      "              \"kwargs\": {\n",
      "                \"column\": \"ID\",\n",
      "                \"batch_id\": \"68a74190e9f8d93771db1ff953848ddd\"\n",
      "              },\n",
      "              \"expectation_type\": \"expect_column_values_to_not_be_null\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"meta\": {},\n",
      "            \"success\": false,\n",
      "            \"exception_info\": {\n",
      "              \"exception_traceback\": \"Traceback (most recent call last):\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\execution_engine\\\\execution_engine.py\\\", line 407, in resolve_metrics\\n    resolved_metrics[metric_to_resolve.id] = metric_fn(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\metrics\\\\metric_provider.py\\\", line 55, in inner_func\\n    return metric_fn(*args, **kwargs)\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\metrics\\\\map_metric_provider.py\\\", line 1375, in inner_func\\n    ) = execution_engine.get_compute_domain(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\execution_engine\\\\sparkdf_execution_engine.py\\\", line 624, in get_compute_domain\\n    data: DataFrame = self.get_domain_records(domain_kwargs=domain_kwargs)\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\execution_engine\\\\sparkdf_execution_engine.py\\\", line 546, in get_domain_records\\n    conditions = [\\nTypeError: 'NoneType' object is not iterable\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\validator\\\\validation_graph.py\\\", line 268, in _resolve\\n    self._execution_engine.resolve_metrics(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\execution_engine\\\\execution_engine.py\\\", line 411, in resolve_metrics\\n    raise ge_exceptions.MetricResolutionError(\\ngreat_expectations.exceptions.exceptions.MetricResolutionError: 'NoneType' object is not iterable\\n\",\n",
      "              \"exception_message\": \"'NoneType' object is not iterable\",\n",
      "              \"raised_exception\": true\n",
      "            },\n",
      "            \"result\": {},\n",
      "            \"expectation_config\": {\n",
      "              \"meta\": {},\n",
      "              \"kwargs\": {\n",
      "                \"column\": \"ID\",\n",
      "                \"batch_id\": \"68a74190e9f8d93771db1ff953848ddd\"\n",
      "              },\n",
      "              \"expectation_type\": \"expect_compound_columns_to_be_unique\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"meta\": {},\n",
      "            \"success\": false,\n",
      "            \"exception_info\": {\n",
      "              \"exception_traceback\": \"Traceback (most recent call last):\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\core\\\\expect_column_values_to_be_of_type.py\\\", line 202, in validate_configuration\\n    assert \\\"type_\\\" in configuration.kwargs, \\\"type_ is required\\\"\\nAssertionError: type_ is required\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\validator\\\\validator.py\\\", line 1051, in graph_validate\\n    result = configuration.metrics_validate(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\core\\\\expectation_configuration.py\\\", line 1404, in metrics_validate\\n    return expectation_impl(self).metrics_validate(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\expectation.py\\\", line 255, in __init__\\n    self.validate_configuration(configuration=configuration)\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\core\\\\expect_column_values_to_be_of_type.py\\\", line 204, in validate_configuration\\n    raise InvalidExpectationConfigurationError(str(e))\\ngreat_expectations.exceptions.exceptions.InvalidExpectationConfigurationError: type_ is required\\n\",\n",
      "              \"exception_message\": \"type_ is required\",\n",
      "              \"raised_exception\": true\n",
      "            },\n",
      "            \"result\": {},\n",
      "            \"expectation_config\": {\n",
      "              \"meta\": {},\n",
      "              \"kwargs\": {\n",
      "                \"column\": \"name\",\n",
      "                \"datatype\": \"str\",\n",
      "                \"batch_id\": \"68a74190e9f8d93771db1ff953848ddd\"\n",
      "              },\n",
      "              \"expectation_type\": \"expect_column_values_to_be_of_type\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"meta\": {},\n",
      "            \"success\": false,\n",
      "            \"exception_info\": {\n",
      "              \"exception_traceback\": \"Traceback (most recent call last):\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\core\\\\expect_column_values_to_be_in_set.py\\\", line 437, in validate_configuration\\n    assert isinstance(\\nAssertionError: value_set must be a list, set, or dict\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\validator\\\\validator.py\\\", line 1051, in graph_validate\\n    result = configuration.metrics_validate(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\core\\\\expectation_configuration.py\\\", line 1404, in metrics_validate\\n    return expectation_impl(self).metrics_validate(\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\expectation.py\\\", line 255, in __init__\\n    self.validate_configuration(configuration=configuration)\\n  File \\\"C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\core\\\\expect_column_values_to_be_in_set.py\\\", line 445, in validate_configuration\\n    raise InvalidExpectationConfigurationError(str(e))\\ngreat_expectations.exceptions.exceptions.InvalidExpectationConfigurationError: value_set must be a list, set, or dict\\n\",\n",
      "              \"exception_message\": \"value_set must be a list, set, or dict\",\n",
      "              \"raised_exception\": true\n",
      "            },\n",
      "            \"result\": {},\n",
      "            \"expectation_config\": {\n",
      "              \"meta\": {},\n",
      "              \"kwargs\": {\n",
      "                \"column\": \"gender\",\n",
      "                \"value_set\": \"['Male','Female']\",\n",
      "                \"expect_column_values_to_contain_valid_email\": {\n",
      "                  \"column\": \"email\"\n",
      "                },\n",
      "                \"batch_id\": \"68a74190e9f8d93771db1ff953848ddd\"\n",
      "              },\n",
      "              \"expectation_type\": \"expect_column_values_to_be_in_set\"\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"statistics\": {\n",
      "          \"evaluated_expectations\": 4,\n",
      "          \"successful_expectations\": 0,\n",
      "          \"unsuccessful_expectations\": 4,\n",
      "          \"success_percent\": 0.0\n",
      "        }\n",
      "      },\n",
      "      \"actions_results\": {\n",
      "        \"store_validation_result\": {\n",
      "          \"class\": \"StoreValidationResultAction\"\n",
      "        },\n",
      "        \"store_evaluation_params\": {\n",
      "          \"class\": \"StoreEvaluationParametersAction\"\n",
      "        },\n",
      "        \"update_data_docs\": {\n",
      "          \"local_site\": null,\n",
      "          \"class\": \"UpdateDataDocsAction\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"evaluation_parameters\": null,\n",
      "  \"validation_operator_config\": {\n",
      "    \"class_name\": \"ActionListValidationOperator\",\n",
      "    \"module_name\": \"great_expectations.validation_operators\",\n",
      "    \"name\": \"action_list_operator\",\n",
      "    \"kwargs\": {\n",
      "      \"action_list\": [\n",
      "        {\n",
      "          \"name\": \"store_validation_result\",\n",
      "          \"action\": {\n",
      "            \"class_name\": \"StoreValidationResultAction\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"store_evaluation_params\",\n",
      "          \"action\": {\n",
      "            \"class_name\": \"StoreEvaluationParametersAction\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"update_data_docs\",\n",
      "          \"action\": {\n",
      "            \"class_name\": \"UpdateDataDocsAction\"\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"result_format\": {\n",
      "        \"result_format\": \"SUMMARY\",\n",
      "        \"partial_unexpected_count\": 20\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "suit_lst = []\n",
    "col_expectations = {\"expect_column_values_to_not_be_null\":{\"column\":\"ID\"},\"expect_compound_columns_to_be_unique\":{\"column\":\"ID\"},\"expect_column_values_to_be_of_type\":{\"column\":\"name\",\"datatype\":\"str\"},\"expect_column_values_to_be_in_set\":{\"column\":\"gender\",\"value_set\":\"['Male','Female']\",\"expect_column_values_to_contain_valid_email\":{\"column\":\"email\"}}}\n",
    "\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "for k,v in col_expectations.items():\n",
    "    expectation_configuration = ExpectationConfiguration(\n",
    "    expectation_type =k, kwargs = v\n",
    "    )\n",
    "    suit_lst.append(expectation_configuration)\n",
    "\n",
    "suite = ExpectationSuite(\"custom_expec_test\",expectations=suit_lst)\n",
    "\n",
    "validator_list = []\n",
    "validator = context.get_validator(batch_request= batch_request, expectation_suite= suite)\n",
    "\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "validator_list.append(validator)\n",
    "results = context.run_validation_operator(\"action_list_operator\",assets_to_validate=validator_list)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef08dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
